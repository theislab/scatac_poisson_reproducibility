# Experiment configuration file.
#
# There are two special blocks. The 'seml' block is required for every experiment.
# It has to contain the following values:
# executable:        Name of the Python script containing the experiment. The path should be relative to the `project_root_dir`.
#                    For backward compatibility SEML also supports paths relative to the location of the config file.
#                    In case there are files present both relative to the project root and the config file,
#                    the former takes precedence.
# It can optionally also contain the following values:
# name:              Prefix for output file and Slurm job name. Default: Collection name
# output_dir:        Directory to store log files in. Default: Current directory
# conda_environment: Specifies which Anaconda virtual environment will be activated before the experiment is executed.
#                    Default: The environment used when queuing.
# project_root_dir:  (Relative or absolute) path to the root of the project. seml will then upload all the source
#                    files imported by the experiment to the MongoDB. Moreover, the uploaded source files will be
#                    downloaded before starting an experiment, so any changes to the source files in the project
#                    between queueing and starting the experiment will have no effect.
#
# The special 'slurm' block contains the slurm parameters. This block and all values are optional. Possible values are:
# experiments_per_job:     Number of parallel experiments to run in each Slurm job.
#                          Note that only experiments from the same batch share a job. Default: 1
# max_simultaneous_jobs:   Maximum number of simultaneously running Slurm jobs per job array. Default: No restriction
# sbatch_options_template: Name of a custom template of `SBATCH` options. Define your own templates in `settings.py`
#                          under `SBATCH_OPTIONS_TEMPLATES`, e.g. for long-running jobs, CPU-only jobs, etc.
# sbatch_options:          dictionary that contains custom values that will be passed to `sbatch`, specifying e.g.
#                          the memory and number of GPUs to be allocated (prepended dashes are not required). See
#                          https://slurm.schedmd.com/sbatch.html for all possible options.
#
# Parameters under 'fixed' will be used for all the experiments.
#
# Under 'grid' you can define parameters that should be sampled from a regular grid. Options are:
#   - choice:     List the different values you want to evaluate under 'choices' as in the example below.
#   - range:      Specify the min, max, and step. Parameter values will be generated using np.arange(min, max, step).
#   - uniform:    Specify the min, max, and num. Parameter values will be generated using
#                 np.linspace(min, max, num, endpoint=True)
#   - loguniform: Specify min, max, and num. Parameter values will be uniformly generated in log space (base 10).
#
# Under 'random' you can specify parameters for which you want to try several random values. Specify the number
# of samples per parameter with the 'samples' value as in the examples below.
# Specify the the seed under the 'random' dict or directly for the desired parameter(s).
# Supported parameter types are:
#   - choice:      Randomly samples <samples> entries (with replacement) from the list in parameter['options']
#   - uniform:     Uniformly samples between 'min' and 'max' as specified in the parameter dict.
#   - loguniform:  Uniformly samples in log space between 'min' and 'max' as specified in the parameter dict.
#   - randint:     Randomly samples integers between 'min' (included) and 'max' (excluded).
#
# The configuration file can be nested (as the example below) so that we can run different parameter sets
# e.g. for different datasets or models.
# We take the cartesian product of all `grid` parameters on a path and sample all random parameters on the path.
# The number of random parameters sampled will be max{n_samples} of all n_samples on the path. This is done because
# we need the same number of samples from all random parameters in a configuration.
#
# More specific settings (i.e., further down the hierarchy) always overwrite more general ones.


seml:
  executable: poisson_atac/seml/atac_to_atac/cross_validation/cv_atac_to_atac.py
  name: ATAC_ATAC_prediction
  output_dir: poisson_atac/seml/logs
  project_root_dir: /storage/groups/ml01/code/laura.martens/atac_poisson_study/
  conda_environment: poisson_atac_new
slurm:
  experiments_per_job: 1
  max_simultaneous_jobs: 10
  sbatch_options_template: GPU
  sbatch_options:
     gres: gpu:1       # num GPUs
     mem: 45G          # memory
     cpus-per-task: 4  # num cores
     time: 0-20:01     # max time, D-HH:MM

###### BEGIN PARAMETER CONFIGURATION ######

fixed:
  training.max_epochs: 500
  training.save_path: "/storage/groups/ml01/workspace/laura.martens/atac_poisson_data/models/seml"
  training.project_name: "atac_to_atac"
  optimization.regularization:
    weight_decay: 1e-3
    learning_rate: 1e-4

#Intial sweep for finding optimal n_latent
Initial_sweep:
  fixed:
    #model
    scvi.seed: 0

    #data
    data.dataset: "neurips" #only testing on Neurips
    setup.batch_key: "batch"
    setup.label_key: "cell_type"
    data.batch: ['s1d1','s1d2','s1d3','s2d1','s2d5','s3d10','s3d3','s3d6','s3d7','s2d4'] # all training batches

  grid:
    setup.model_params.n_latent:
      type: range
      min: 10
      max: 101
      step: 10

  Poissonvi:
    fixed:
      model.model_type: "poissonvi"
      setup.layer: "counts" #using count layer
    grid:
      setup.model_params.use_observed_lib_size: 
        type: choice
        options:
        - False  #Using encoded library size
        - True   #Using true library size
      
  Peakvi:
    fixed:
      model.model_type: "peakvi"
      setup.layer: None #using binary data

  Binaryvi:
      fixed:
        model.model_type: "binaryvi"
        setup.layer: None #using binary data

#Intial sweep for finding optimal n_latent
Initial_sweep_Satpathy:
  fixed:
    #model
    scvi.seed: 0

    #data
    data.dataset: "hematopoiesis" #only testing on Neurips
    setup.batch_key: "Group"
    setup.label_key: "cell_type"
    data.batch: ['PBMC_Rep1', 'PBMC_Rep2', 'PBMC_Rep3', 'PBMC_Rep4', 'Bone_Marrow_Rep1'] # some batches that are 40% of complete dataset

  grid:
    setup.model_params.n_latent:
      type: range
      min: 10
      max: 101
      step: 10

  Poissonvi:
    fixed:
      model.model_type: "poissonvi"
      setup.layer: "counts" #using count layer
    grid:
      setup.model_params.use_observed_lib_size: 
        type: choice
        options:
        - False  #Using encoded library size
        - True   #Using true library size
      
  Peakvi:
    fixed:
      model.model_type: "peakvi"
      setup.layer: None #using binary data

  Binaryvi:
      fixed:
        model.model_type: "binaryvi"
        setup.layer: None #using binary data


# Evaluation over multiple train,val, test sets for statistics using all batches
# Using best n_latent from initial sweep: see notebooks
Cross_validation_NeurIPS:
  fixed:
    #data
    data.dataset: "neurips" #only testing on Neurips
    setup.batch_key: "batch"
    setup.label_key: "cell_type"
    data.batch: None # all training & test batches

  grid:
    scvi.seed:
      type: choice
      options: 
          - 0
          - 1
          - 2
          - 3
          - 4

  Poissonvi_observed:
    fixed:
      model.model_type: "poissonvi"
      setup.layer: "counts" #using count layer
      setup.model_params.n_latent: 100
      setup.model_params.use_observed_lib_size: True

  Poissonvi_encoded:
    fixed:
      model.model_type: "poissonvi"
      setup.layer: "counts" #using count layer
      setup.model_params.n_latent: 70
      setup.model_params.use_observed_lib_size: False

  Peakvi:
    fixed:
      model.model_type: "peakvi"
      setup.layer: None #using binary data
      setup.model_params.n_latent: 80

  Binaryvi_observed:
    fixed:
      model.model_type: "binaryvi"
      setup.layer: None #using binary data
      setup.model_params.n_latent: 60
      setup.model_params.use_observed_lib_size: True

  Binaryvi_encoded:
    fixed:
      model.model_type: "binaryvi"
      setup.layer: None #using binary data
      setup.model_params.n_latent: 60
      setup.model_params.use_observed_lib_size: False


# Evaluation over multiple train,val, test sets for statistics using all batches
# Using best n_latent from initial sweep: see notebooks
Cross_validation_Satpathy:
  fixed:
    #data
    data.dataset: "hematopoiesis"
    setup.label_key: "cell_type"
    setup.batch_key: "Group"

  grid:
    data.batch: 
     type: choice
     options:
      - ['PBMC_Rep1', 'PBMC_Rep2', 'PBMC_Rep3', 'PBMC_Rep4'] # For integration metrics
      - None
    scvi.seed:
      type: choice
      options: 
          - 0
          - 1
          - 2
          - 3
          - 4

  Poissonvi_observed:
    fixed:
      model.model_type: "poissonvi"
      setup.layer: "counts" #using count layer
      setup.model_params.n_latent: 100
      setup.model_params.use_observed_lib_size: True

  Poissonvi_encoded:
    fixed:
      model.model_type: "poissonvi"
      setup.layer: "counts" #using count layer
      setup.model_params.n_latent: 60
      setup.model_params.use_observed_lib_size: False

  Peakvi:
    fixed:
      model.model_type: "peakvi"
      setup.layer: None #using binary data
      setup.model_params.n_latent: 70

  Binaryvi_observed:
    fixed:
      model.model_type: "binaryvi"
      setup.layer: None #using binary data
      setup.model_params.n_latent: 100
      setup.model_params.use_observed_lib_size: True

  Binaryvi_encoded:
    fixed:
      model.model_type: "binaryvi"
      setup.layer: None #using binary data
      setup.model_params.n_latent: 100
      setup.model_params.use_observed_lib_size: False
