# Experiment configuration file.
#
# There are two special blocks. The 'seml' block is required for every experiment.
# It has to contain the following values:
# executable:        Name of the Python script containing the experiment. The path should be relative to the `project_root_dir`.
#                    For backward compatibility SEML also supports paths relative to the location of the config file.
#                    In case there are files present both relative to the project root and the config file,
#                    the former takes precedence.
# It can optionally also contain the following values:
# name:              Prefix for output file and Slurm job name. Default: Collection name
# output_dir:        Directory to store log files in. Default: Current directory
# conda_environment: Specifies which Anaconda virtual environment will be activated before the experiment is executed.
#                    Default: The environment used when queuing.
# project_root_dir:  (Relative or absolute) path to the root of the project. seml will then upload all the source
#                    files imported by the experiment to the MongoDB. Moreover, the uploaded source files will be
#                    downloaded before starting an experiment, so any changes to the source files in the project
#                    between queueing and starting the experiment will have no effect.
#
# The special 'slurm' block contains the slurm parameters. This block and all values are optional. Possible values are:
# experiments_per_job:     Number of parallel experiments to run in each Slurm job.
#                          Note that only experiments from the same batch share a job. Default: 1
# max_simultaneous_jobs:   Maximum number of simultaneously running Slurm jobs per job array. Default: No restriction
# sbatch_options_template: Name of a custom template of `SBATCH` options. Define your own templates in `settings.py`
#                          under `SBATCH_OPTIONS_TEMPLATES`, e.g. for long-running jobs, CPU-only jobs, etc.
# sbatch_options:          dictionary that contains custom values that will be passed to `sbatch`, specifying e.g.
#                          the memory and number of GPUs to be allocated (prepended dashes are not required). See
#                          https://slurm.schedmd.com/sbatch.html for all possible options.
#
# Parameters under 'fixed' will be used for all the experiments.
#
# Under 'grid' you can define parameters that should be sampled from a regular grid. Options are:
#   - choice:     List the different values you want to evaluate under 'choices' as in the example below.
#   - range:      Specify the min, max, and step. Parameter values will be generated using np.arange(min, max, step).
#   - uniform:    Specify the min, max, and num. Parameter values will be generated using
#                 np.linspace(min, max, num, endpoint=True)
#   - loguniform: Specify min, max, and num. Parameter values will be uniformly generated in log space (base 10).
#
# Under 'random' you can specify parameters for which you want to try several random values. Specify the number
# of samples per parameter with the 'samples' value as in the examples below.
# Specify the the seed under the 'random' dict or directly for the desired parameter(s).
# Supported parameter types are:
#   - choice:      Randomly samples <samples> entries (with replacement) from the list in parameter['options']
#   - uniform:     Uniformly samples between 'min' and 'max' as specified in the parameter dict.
#   - loguniform:  Uniformly samples in log space between 'min' and 'max' as specified in the parameter dict.
#   - randint:     Randomly samples integers between 'min' (included) and 'max' (excluded).
#
# The configuration file can be nested (as the example below) so that we can run different parameter sets
# e.g. for different datasets or models.
# We take the cartesian product of all `grid` parameters on a path and sample all random parameters on the path.
# The number of random parameters sampled will be max{n_samples} of all n_samples on the path. This is done because
# we need the same number of samples from all random parameters in a configuration.
#
# More specific settings (i.e., further down the hierarchy) always overwrite more general ones.


seml:
  executable: poisson_atac/seml/atac_to_atac/cross_validation/cv_atac_to_atac.py
  name: ATAC_ATAC_prediction
  output_dir: poisson_atac/seml/logs
  project_root_dir: /storage/groups/ml01/code/laura.martens/atac_poisson_study/
  conda_environment: poisson_atac_new
slurm:
  experiments_per_job: 1
  max_simultaneous_jobs: 10
  sbatch_options_template: GPU
  sbatch_options:
     gres: gpu:1       # num GPUs
     mem: 30G          # memory
     cpus-per-task: 4  # num cores
     time: 0-20:01     # max time, D-HH:MM

###### BEGIN PARAMETER CONFIGURATION ######

fixed:
  training.max_epochs: 500
  training.save_path: "/storage/groups/ml01/workspace/laura.martens/atac_poisson_data/models/seml"
  training.project_name: "cv_atac_to_atac"
  optimization.regularization:
    weight_decay: 1e-3
    learning_rate: 1e-4

grid:
  scvi.seed:
    type: choice
    options: 
        - 0
        - 1
        - 2
        - 3
        - 4

Peakvi:
  fixed:
    setup.model_params.use_layer_norm: "both"
    model.model_type: "peakvi"
    setup.model_params.n_latent: None
    
  neurips:
    fixed:
      data.dataset: "neurips"
      setup.layer: None
      setup.batch_key: "batch"
      setup.label_key: "cell_type"

    grid:
      data.batch: 
        type: choice
        options:
          - ['s1d1','s1d2','s1d3','s2d1','s2d5','s3d10','s3d3','s3d6','s3d7','s2d4']
          - "s1d1"
          - "s2d1"
          - "s3d10"
          - "s1d2"
          - "s1d3"
          - "s2d4"
          - "s2d5"
          - "s3d3"
          - "s3d6"
          - "s3d7"

  hematopoiesis:

    fixed:
      data.dataset: "hematopoiesis"
      setup.layer: None
      setup.label_key: "cell_type"
      data.batch: None
      setup.batch_key: "Group"


Count:

  fixed:
    setup.model_params.use_layer_norm: "both"
    model.model_type: "count"
    setup.model_params.n_latent: None
    
  neurips:
    fixed:
      data.dataset: "neurips"
      setup.layer: "counts"
      setup.batch_key: "batch"
      setup.label_key: "cell_type"

    grid:
      data.batch: 
        type: choice
        options:
          - ['s1d1','s1d2','s1d3','s2d1','s2d5','s3d10','s3d3','s3d6','s3d7','s2d4']
          - "s1d1"
          - "s2d1"
          - "s3d10"
          - "s1d2"
          - "s1d3"
          - "s2d4"
          - "s2d5"
          - "s3d3"
          - "s3d6"
          - "s3d7"

  hematopoiesis:

    fixed:
      data.dataset: "hematopoiesis"
      setup.layer: "counts" 
      setup.label_key: "cell_type"
      data.batch: None
      setup.batch_key: "Group"

